{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "builder = SparkSession.builder.appName(\"Streaming2_practice\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "KAFKA_USER = os.getenv('KAFKA_USER')\n",
    "KAFKA_PW = os.environ['KAFKA_TOPIC'] = 'your_kafka_topic' \n",
    "KAFKA_TOPIC = os.getenv('KAFKA_TOPIC')\n",
    "KAFKA_GROUP_PREFIX = os.getenv('KAFKA_GROUP_PREFIX')\n",
    "KAFKA_JAAS_CONFIG = f\"org.apache.kafka.common.security.scram.ScramLoginModule required username='{KAFKA_USER}' password='{KAFKA_PW}';\"\n",
    "\n",
    "# Generate a random UUID\n",
    "KAFKA_GROUP_ID = f\"{KAFKA_GROUP_PREFIX}{uuid.uuid4()}\"\n",
    "\n",
    "# spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True) # OK for exploration, not great for performance\n",
    "# spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark configuration note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with some performance configurations \n",
    "\n",
    "#spark.conf.get(\"spark.sql.shuffle.partitions\") # this is the number of partitions when shuffling data. By default it is 200, which may be overkill for smaller clusters\n",
    "\n",
    "#spark._sc.defaultParallelism # this is the default level of parallelism based on the system settings. E.g., amount of cores, cluster size, etc\n",
    "\n",
    "# For most, and especially smaller datasets, it makes sense to reduce the shuffle partitions. \n",
    "# However, if you get OOM for huge datasets, you may need to increase the partitions. There is no golden rule, you need to test and benchmark\n",
    "#spark.conf.set(\"spark.sql.shuffle.partitions\", spark._sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8307cf20-40d9-4eef-9d0b-af50f5901b50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Structured Streaming\n",
    "  \n",
    "  \n",
    "* Kafka \n",
    "* Aggregations\n",
    "* Time windows\n",
    "* Watermarking\n",
    "* Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b2068d2-2855-4d9b-a6e1-2e7d57853e53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for getting data from Kafka (or other distributed log systems), we need minimum 2 things:\n",
    "# the server\n",
    "# the topic\n",
    "\n",
    "kafka_server = \"localhost:9093\"  # Correct Kafka bootstrap server\n",
    "\n",
    "orders_df = (spark.readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_server)\n",
    "  .option(\"kafka.security.protocol\", \"SASL_SSL\")  # Or SASL_PLAINTEXT if youâ€™re using SCRAM-SHA-512\n",
    "  .option(\"kafka.sasl.mechanism\", \"SCRAM-SHA-512\")\n",
    "  .option(\"kafka.sasl.jaas.config\", KAFKA_JAAS_CONFIG)\n",
    "  .option(\"kafka.group.id\", KAFKA_GROUP_ID)\n",
    "  .option(\"subscribe\", KAFKA_TOPIC)  # Ensure this matches your producer's topic\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"maxOffsetsPerTrigger\", 100)\n",
    "  .load()\n",
    ")\n",
    "\n",
    "orders_df.printSchema()  # To see the schema of the data being received\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"streaming/orders/_checkpoint\" \n",
    "table_name = \"orders_s\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "orders_delta_query = (orders_df.writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_delta_query\")\n",
    "  .trigger(processingTime=\"5 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "# if you create the table metastore before any data exists then the stream will result in an error as the table is generated with empty schema\n",
    "def create_table_if_exists(output_path,table_name):\n",
    "    data_exists = False\n",
    "    for _i in range(60): # you can replace this with while, currently timeouts after about 60 seconds\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            files = os.listdir(output_path)\n",
    "            for _f in files:\n",
    "                if \".parquet\" in _f:\n",
    "                    if len(os.listdir(f\"{output_path}/_delta_log\"))>0:\n",
    "                        print(\"data exists\")\n",
    "                        data_exists = True\n",
    "                        break\n",
    "            if data_exists:\n",
    "                spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{table_name}'\") # table metastore is created once there is some data (.parquet) in the directory\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e) # if you want to see the exceptions, uncomment this\n",
    "            pass\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot start query with name orders_delta_query as a query with that name is already active in this SparkSession",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 68\u001b[0m\n\u001b[1;32m     59\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Write the stream to Delta format\u001b[39;00m\n\u001b[1;32m     62\u001b[0m orders_delta_query \u001b[38;5;241m=\u001b[39m (\u001b[43morders_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Append mode since we're continuously adding data\u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morders_delta_query\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m5 second\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Trigger interval, adjust as necessary\u001b[39;49;00m\n\u001b[1;32m     67\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Checkpoint location to allow recovery\u001b[39;49;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Delta table output path\u001b[39;00m\n\u001b[1;32m     69\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/readwriter.py:1529\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart())\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot start query with name orders_delta_query as a query with that name is already active in this SparkSession"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "# Create SparkSession with Delta Lake support\n",
    "builder = SparkSession.builder.appName(\"KafkaToDeltaStream\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Load environment variables for Kafka connection\n",
    "load_dotenv()\n",
    "\n",
    "KAFKA_USER = os.getenv('KAFKA_USER')   # Kafka user credentials\n",
    "KAFKA_PW  = os.environ['KAFKA_TOPIC'] = 'your_kafka_topic' \n",
    "KAFKA_TOPIC = os.getenv('KAFKA_TOPIC')  # Kafka topic\n",
    "KAFKA_GROUP_PREFIX = os.getenv('KAFKA_GROUP_PREFIX')  # Kafka group prefix\n",
    "KAFKA_JAAS_CONFIG = f\"org.apache.kafka.common.security.scram.ScramLoginModule required username='{KAFKA_USER}' password='{KAFKA_PW}';\"\n",
    "\n",
    "# Generate a random UUID for group ID\n",
    "KAFKA_GROUP_ID = f\"{KAFKA_GROUP_PREFIX}{uuid.uuid4()}\"\n",
    "\n",
    "# Kafka server configuration\n",
    "kafka_server = \"localhost:9093\"  # Kafka server address\n",
    "\n",
    "# Reading data from Kafka stream\n",
    "orders_df = (spark.readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_server)\n",
    "  .option(\"kafka.security.protocol\", \"SASL_SSL\")  # Adjust according to your security protocol\n",
    "  .option(\"kafka.sasl.mechanism\", \"SCRAM-SHA-512\")  # SASL mechanism for authentication\n",
    "  .option(\"kafka.sasl.jaas.config\", KAFKA_JAAS_CONFIG)\n",
    "  .option(\"kafka.group.id\", KAFKA_GROUP_ID)\n",
    "  .option(\"subscribe\", KAFKA_TOPIC)  # Subscribe to the Kafka topic\n",
    "  .option(\"startingOffsets\", \"earliest\")  # Start from the earliest offset\n",
    "  .option(\"maxOffsetsPerTrigger\", 100)  # Rate limit for offsets per trigger\n",
    "  .load()  # Read the stream\n",
    ")\n",
    "\n",
    "# The data in Kafka is in binary format, we need to decode the 'value' column\n",
    "orders_df = orders_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")  # Decode 'key' and 'value' to strings\n",
    "\n",
    "# Optionally: you can parse the 'value' as JSON if the data is in JSON format\n",
    "orders_df = orders_df.withColumn(\"value\", F.from_json(orders_df[\"value\"], \"order_id INT, order_value INT\"))\n",
    "\n",
    "\n",
    "\n",
    "# Output path and checkpoint location\n",
    "checkpoint_path = \"streaming/orders/_checkpoint\"\n",
    "output_path = \"delta/orders_output\"\n",
    "\n",
    "# Ensure output path exists (optional, you can handle it dynamically)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Write the stream to Delta format\n",
    "orders_delta_query = (orders_df.writeStream\n",
    "  .outputMode(\"append\")  # Append mode since we're continuously adding data\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_delta_query\")\n",
    "  .trigger(processingTime=\"5 second\")  # Trigger interval, adjust as necessary\n",
    "  .option(\"checkpointLocation\", checkpoint_path)  # Checkpoint location to allow recovery\n",
    "  .start(output_path)  # Delta table output path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `orders_s` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [orders_s], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m create_table_if_exists(output_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morders_s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Display the first 5 rows of the table\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m display(\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morders_s\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     33\u001b[0m display(spark\u001b[38;5;241m.\u001b[39mtable(table_name)\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#key - the data key. Used in state machines, not useful in this case\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#value - the data, in binary format. This is our JSON payload. We'll need to cast it to STRING.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#topic - the topic we are subscribing to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#timestamp - the timestamp (commonly, the processing (or ingestion) timestamp)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#timestampType - whether timestamp is create time or log append time \u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1667\u001b[0m, in \u001b[0;36mSparkSession.table\u001b[0;34m(self, tableName)\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtable\u001b[39m(\u001b[38;5;28mself\u001b[39m, tableName: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m \n\u001b[1;32m   1639\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;124;03m    +---+\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `orders_s` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [orders_s], [], false\n"
     ]
    }
   ],
   "source": [
    "# let's have a look at the data \n",
    "\n",
    "\n",
    "# Optional: Function to create table if not exists\n",
    "def create_table_if_exists(output_path, table_name):\n",
    "    data_exists = False\n",
    "    for _i in range(60):  # Check for data presence\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            files = os.listdir(output_path)\n",
    "            for _f in files:\n",
    "                if \".parquet\" in _f:\n",
    "                    if len(os.listdir(f\"{output_path}/_delta_log\")) > 0:\n",
    "                        print(\"Data exists in Delta path.\")\n",
    "                        data_exists = True\n",
    "                        break\n",
    "            if data_exists:\n",
    "                # Create the table only when data exists\n",
    "                spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{output_path}'\")\n",
    "                print(f\"Table {table_name} created.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "# Create the table once data is available\n",
    "create_table_if_exists(output_path, \"orders_s\")\n",
    "\n",
    "# Display the first 5 rows of the table\n",
    "display(spark.table(\"orders_s\").limit(5))\n",
    "\n",
    "\n",
    "\n",
    "display(spark.table(table_name).limit(5))\n",
    "\n",
    "#key - the data key. Used in state machines, not useful in this case\n",
    "#value - the data, in binary format. This is our JSON payload. We'll need to cast it to STRING.\n",
    "#topic - the topic we are subscribing to\n",
    "#partition - partition.\n",
    "#offset - the offset value. This is per topic, partition, and consumer group\n",
    "#timestamp - the timestamp (commonly, the processing (or ingestion) timestamp)\n",
    "#timestampType - whether timestamp is create time or log append time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a522a2-5e9e-4f8d-a9f2-f5db0012b652",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `orders_s` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [orders_s], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# let's have a look at the JSON payload\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringType\n\u001b[0;32m----> 5\u001b[0m orders_payload_df \u001b[38;5;241m=\u001b[39m (\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m                   \u001b[38;5;241m.\u001b[39mselect(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(StringType()))\n\u001b[1;32m      7\u001b[0m                   )\n\u001b[1;32m      9\u001b[0m display(orders_payload_df)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1667\u001b[0m, in \u001b[0;36mSparkSession.table\u001b[0;34m(self, tableName)\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtable\u001b[39m(\u001b[38;5;28mself\u001b[39m, tableName: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m \n\u001b[1;32m   1639\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;124;03m    +---+\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `orders_s` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [orders_s], [], false\n"
     ]
    }
   ],
   "source": [
    "# let's have a look at the JSON payload\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "orders_payload_df = (spark.table(table_name)\n",
    "                  .select(F.col(\"value\").cast(StringType()))\n",
    "                  )\n",
    "\n",
    "display(orders_payload_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "401c2488-760f-4374-930e-d2326fc4e632",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's create a schema for navigating the JSON payload\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"order_id\", StringType(), True),\n",
    "  StructField(\"user_id\", StringType(), True),\n",
    "  StructField(\"product_ids\", ArrayType(StringType(), True), True),\n",
    "  StructField(\"order_timestamp\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17528c23-8656-4562-9fc7-11b2ba6eb7af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'orders_payload_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now we can use \"from_json\" to parse out the message and provide schema\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m orders_json_df \u001b[38;5;241m=\u001b[39m (\u001b[43morders_payload_df\u001b[49m\n\u001b[1;32m      4\u001b[0m                 \u001b[38;5;241m.\u001b[39mselect(F\u001b[38;5;241m.\u001b[39mfrom_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, schema)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      5\u001b[0m                 \u001b[38;5;241m.\u001b[39mselect(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson.*\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      6\u001b[0m                )\n\u001b[1;32m      8\u001b[0m display(orders_json_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'orders_payload_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Now we can use \"from_json\" to parse out the message and provide schema\n",
    "\n",
    "orders_json_df = (orders_payload_df\n",
    "                .select(F.from_json(\"value\", schema).alias(\"json\"))\n",
    "                .select(F.col(\"json.*\"))\n",
    "               )\n",
    "\n",
    "display(orders_json_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's cast the ids into integer and create a new df that has the clean payload data \n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "orders_cleaned_df = (orders_df\n",
    "  .select(F.col(\"value\").cast(StringType()))\n",
    "  .select(F.from_json(\"value\", schema).alias(\"json\"))\n",
    "  .select(F.col(\"json.*\"))\n",
    "  .withColumn(\"order_id\", F.col(\"order_id\").cast(IntegerType()))\n",
    "  .withColumn(\"user_id\", F.col(\"user_id\").cast(IntegerType()))\n",
    "  .withColumn(\"product_ids\", F.transform(F.col(\"product_ids\"), lambda x: x.cast(IntegerType())))\n",
    ")\n",
    "\n",
    "orders_cleaned_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's cast the ids into integer and create a new query that outputs the users by most purchased products \n",
    "checkpoint_path = \"streaming/orders_most_products/_checkpoint\" \n",
    "table_name = \"orders_most_products\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "orders_most_products_query = (orders_cleaned_df\n",
    "  .select(\"user_id\",F.size(\"product_ids\").alias(\"count_of_products\"))\n",
    "  .groupBy(\"user_id\")\n",
    "  .sum(\"count_of_products\")\n",
    "  .withColumnRenamed(\"sum(count_of_products)\", \"product_count\")\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_most_products_query\")\n",
    "  .trigger(processingTime=\"5 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 77c385a9-470d-4cd4-b424-aca160083b69, runId = 9d640a41-25b8-43b7-a30c-030fcff267e1] terminated with exception: Wrong basePath input/sensor-data for the root path: file:/home/jovyan/input/mock_data.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 52\u001b[0m\n\u001b[1;32m     42\u001b[0m csv_delta_query \u001b[38;5;241m=\u001b[39m (df\u001b[38;5;241m.\u001b[39mwriteStream\n\u001b[1;32m     43\u001b[0m                    \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Append mode since we're continuously adding data\u001b[39;00m\n\u001b[1;32m     44\u001b[0m                    \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m                    \u001b[38;5;241m.\u001b[39mstart(output_path)  \u001b[38;5;66;03m# Delta table output path\u001b[39;00m\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Wait for the query to terminate\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mcsv_delta_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = 77c385a9-470d-4cd4-b424-aca160083b69, runId = 9d640a41-25b8-43b7-a30c-030fcff267e1] terminated with exception: Wrong basePath input/sensor-data for the root path: file:/home/jovyan/input/mock_data.csv"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from delta import *\n",
    "import os\n",
    "\n",
    "# Create SparkSession with Delta Lake support\n",
    "builder = SparkSession.builder.appName(\"CSVStreamToDelta\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Define the schema for the CSV file to match the data structure\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"ip_address\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Path to the directory where CSV files are located (for streaming)\n",
    "csv_path = \"input/sensor-data/\"  # Path to the folder containing CSV files\n",
    "\n",
    "# Reading the data as a stream from CSV files in the directory\n",
    "df = (spark.readStream\n",
    "      .schema(schema)  # Define the schema explicitly\n",
    "      .csv(csv_path)   # Read CSV files as a stream from the directory\n",
    ")\n",
    "\n",
    "# Output path and checkpoint location\n",
    "checkpoint_path = \"streaming/csv_checkpoint\"\n",
    "output_path = \"delta/csv_output\"\n",
    "\n",
    "# Ensure output path exists (optional, you can handle it dynamically)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Write the stream to Delta format\n",
    "csv_delta_query = (df.writeStream\n",
    "                   .outputMode(\"append\")  # Append mode since we're continuously adding data\n",
    "                   .format(\"delta\")\n",
    "                   .queryName(\"csv_to_delta_query\")\n",
    "                   .trigger(processingTime=\"5 seconds\")  # Trigger interval, adjust as necessary\n",
    "                   .option(\"checkpointLocation\", checkpoint_path)  # Checkpoint location for fault tolerance\n",
    "                   .start(output_path)  # Delta table output path\n",
    ")\n",
    "\n",
    "# Wait for the query to terminate\n",
    "csv_delta_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "475b45b8-9b3d-412e-9da5-0291489a5214",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_cleaned_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d432333a-1335-4fc4-8b5d-1e79f16ebbf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# commonly, you might not want an aggregation of a stream's whole history.\n",
    "# for this purpose, let's use window from functions - NB this is \"time windows\" not \"SQL-like (row) window function\"\n",
    "\n",
    "checkpoint_path = \"streaming/orders_most_products_tumb/_checkpoint\" \n",
    "table_name = \"orders_most_products_tumb\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "orders_most_products_tumb_query = (orders_cleaned_df\n",
    "  .select(\"user_id\",F.size(\"product_ids\").alias(\"count_of_products\"), \"order_timestamp\")\n",
    "  .groupBy(\"user_id\", F.window(\"order_timestamp\", \"5 minute\")) # Aggregate by user, every 5 minute block. This is a \"tumbling window\"\n",
    "  .sum(\"count_of_products\")\n",
    "  .withColumnRenamed(\"sum(count_of_products)\", \"product_count\")\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_most_products_tumb_query\")\n",
    "  .trigger(processingTime=\"5 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.desc(\"window.end\"),F.desc(\"product_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e86c785c-51ac-4780-be1e-184e01fee543",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if we want to keep always the latest time window then we can use sliding windows\n",
    "\n",
    "checkpoint_path = \"streaming/orders_most_products_slide/_checkpoint\" \n",
    "table_name = \"orders_most_products_slide\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "orders_most_products_slide_query = (orders_cleaned_df\n",
    "  .select(\"user_id\",F.size(\"product_ids\").alias(\"count_of_products\"), \"order_timestamp\")\n",
    "  .groupBy(\"user_id\", F.window(\"order_timestamp\", \"5 minute\", \"1 minute\")) # Aggregate by user, every 5 minute block sliding by 1 minute.\n",
    "  .sum(\"count_of_products\")\n",
    "  .withColumnRenamed(\"sum(count_of_products)\", \"product_count\")\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_most_products_slide_query\")\n",
    "  .trigger(processingTime=\"5 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.desc(\"window.end\"),F.desc(\"product_count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "051067d9-2417-4562-bf14-5f55b0347200",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Non-Kafka part starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acac1e02-d555-4fdc-aa57-0eb83b64c55d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's try with a different, simpler dataset\n",
    "\n",
    "input_path = \"sensor-data\"\n",
    "\n",
    "json_schema = \"time timestamp, action string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f36054-4522-42b8-97aa-7fabeede5da0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a dataframe and apply some transformations and aggregation\n",
    "\n",
    "input_df = (spark\n",
    "  .readStream                                 \n",
    "  .schema(json_schema)                       \n",
    "  .option(\"maxFilesPerTrigger\", 1)            \n",
    "  .json(input_path)                           \n",
    ")\n",
    "\n",
    "counts_df = (input_df\n",
    "  .groupBy(F.col(\"action\"),                     # Aggregate by action\n",
    "           F.window(F.col(\"time\"), \"1 hour\"))     # and by a 1 hour window\n",
    "  .count()                                    # Count the actions\n",
    "  .select(F.col(\"window.start\").alias(\"start\"), \n",
    "          F.col(\"count\"),                       \n",
    "          F.col(\"action\"))                      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aea650a6-ed4e-44a2-8eb7-65d73326a046",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"streaming/counts/_checkpoint\" \n",
    "table_name = \"counts\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "counts_query = (counts_df\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"counts_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.col(\"start\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e200d92c-a0ce-4b7c-be79-3658e2e56dd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Watermarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d968b6b-01a9-480a-8d63-b3e5c0ca159a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# in actual use cases, the queries above would keep running for a very long time and the amount of windows would grow indefinitely\n",
    "# keeping track of all the states puts pressure on memory\n",
    "# also it may often be irrelevant if delayed data updates our figures\n",
    "\n",
    "watermarked_stream = \"watermarked_stream\"\n",
    "\n",
    "watermarked_df = (input_df\n",
    "  .withWatermark(\"time\", \"2 hours\")             # Specify a 2-hour watermark\n",
    "  .groupBy(F.col(\"action\"),                       # Aggregate by action...\n",
    "           F.window(F.col(\"time\"), \"1 hour\"))       # ...then by a 1 hour window\n",
    "  .count()                                      # For each aggregate, produce a count\n",
    "  .select(F.col(\"window.start\").alias(\"start\"),   # Elevate field to column\n",
    "          F.col(\"count\"),                         # Include count\n",
    "          F.col(\"action\"))                        # Include action\n",
    ")\n",
    "display(watermarked_df, streamName = watermarked_stream) # Start the stream and display it\n",
    "\n",
    "# important note: watermarking guarantees that any event within the window gets in. It does not guarantee leaving anything out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in actual use cases, the queries above would keep running for a very long time and the amount of windows would grow indefinitely\n",
    "# keeping track of all the states puts pressure on memory\n",
    "# also it may often be irrelevant if delayed data updates our figures\n",
    "\n",
    "checkpoint_path = \"streaming/orders_most_products_slide_wm/_checkpoint\" \n",
    "table_name = \"orders_most_products_slide_wm\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "orders_most_products_slide_wm_query = (orders_cleaned_df\n",
    "  .withWatermark(\"order_timestamp\", \"20 minute\")             # Specify a 20-minute watermark\n",
    "  .select(\"user_id\",F.size(\"product_ids\").alias(\"count_of_products\"), \"order_timestamp\")\n",
    "  .groupBy(\"user_id\", F.window(\"order_timestamp\", \"5 minute\", \"1 minute\")) # Aggregate by user, every 5 minute block sliding by 1 minute.\n",
    "  .sum(\"count_of_products\")\n",
    "  .withColumnRenamed(\"sum(count_of_products)\", \"product_count\")\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_most_products_slide_wm_query\")\n",
    "  .trigger(processingTime=\"5 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)\n",
    "\n",
    "# important note: watermarking guarantees that any event within the window gets in. It does not guarantee leaving anything out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.desc(\"window.end\"),F.desc(\"product_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4aa920-978f-4e07-ab18-efca64d10f51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's import another dataset. Let's say we are interested in hourly monitoring of incoming traffic to our website\n",
    "\n",
    "schema = \"device STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\"\n",
    "\n",
    "hourlyEventsPath = \"events20200703\"\n",
    "\n",
    "website_df = (spark.readStream\n",
    "  .schema(schema)\n",
    "  .option(\"maxFilesPerTrigger\", 1)\n",
    "  .json(hourlyEventsPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81d948ed-3a26-45e7-8e92-836cc07ae9c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this dataframe does not have a proper timestamp column. So we need to create one and use it for watermarking\n",
    "\n",
    "events_df = (website_df\n",
    "             .withColumn(\"createdAt\", (F.col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n",
    "             .withWatermark(\"createdAt\", \"2 hours\")\n",
    ")             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7b62a6-8a6d-41bc-a226-5c5d7cf40943",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now we can do an aggregation\n",
    "\n",
    "traffic_df = (events_df\n",
    "             .groupBy(\"traffic_source\"\n",
    "                      , F.window(F.col(\"createdAt\"), \"1 hour\"))\n",
    "             .agg(F.approx_count_distinct(\"user_id\").alias(\"active_users\"))\n",
    "             .select(F.col(\"traffic_source\")\n",
    "                     , F.col(\"active_users\")\n",
    "                     , F.hour(F.col(\"window.start\")).alias(\"hour\"))\n",
    "             .sort(\"hour\")\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint_path = \"streaming/traffic/_checkpoint\" \n",
    "table_name = \"traffic\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "traffic_query = (traffic_df\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"traffic_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "307411ab-59c4-4026-b6d8-65b745ca1d28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Joining streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2d54db-d035-4163-a9af-679884a3edb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's load in users dataset\n",
    "\n",
    "users_df = spark.read.parquet(\"users.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "600645dc-8cb3-47ba-8d28-bc6330cefdb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# join works same way as with regular dataframes.\n",
    "# note: this is streaming<->static join\n",
    "\n",
    "joined_df = (events_df\n",
    "            .join(users_df.drop(\"user_first_touch_timestamp\"), \"user_id\")\n",
    "            )\n",
    "\n",
    "checkpoint_path = \"streaming/join_static/_checkpoint\" \n",
    "table_name = \"join_static\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "join_static_query = (joined_df\n",
    "  .writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"join_static_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da8f8dbd-c601-4ed6-b96a-0af6abbabf36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's read in users dataframe as a stream\n",
    "\n",
    "# since we have created the dataframe from this data, we can cheat on getting the schema. Possible in development/debugging, not possible or recommended in production\n",
    "users_schema = users_df.schema\n",
    "\n",
    "users_stream_df = (spark\n",
    "                   .readStream\n",
    "                   .format(\"parquet\")\n",
    "                   .schema(users_schema)\n",
    "                   .option(\"maxFilesPerTrigger\", 1)\n",
    "                   .parquet(\"users.parquet\")\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab5049a-272a-468b-a80a-0b25830b93fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's do a stream to stream join\n",
    "\n",
    "joined_streams_df = (events_df\n",
    "            .join(users_stream_df.drop(\"user_first_touch_timestamp\"), \"user_id\")\n",
    "            )\n",
    "\n",
    "checkpoint_path = \"streaming/join_stream/_checkpoint\" \n",
    "table_name = \"join_stream\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "join_stream_query = (joined_df\n",
    "  .writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"join_stream_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ddcecc-b7c5-427e-bbd7-fea0f4886160",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for stream in spark.streams.active:\n",
    "  stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2ffb003-abb3-4689-9ad0-8564ac25e1cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Further reading\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.window.html  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withWatermark.html  \n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html  \n",
    "https://docs.databricks.com/spark/latest/structured-streaming/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b127445-2b2f-49fb-84fe-fc9063f2450f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Task 1\n",
    "\n",
    "Create a streaming dataframe from the data in the following path:  \n",
    "`flights200701stream`\n",
    "\n",
    "The schema should contain\n",
    "* DepartureAt (timestamp)\n",
    "* UniqueCarrier (string)\n",
    "\n",
    "Process only 1 file per trigger.  \n",
    "\n",
    "Aggregate the data by count, using non-overlapping 30 minute windows.  \n",
    "Ignore any data that is older than 6 hours.\n",
    "\n",
    "The output should have 3 columns: startTime (window start time), UniqueCarrier, count.  \n",
    "\n",
    "Save to a delta table, firing the trigger every 5 seconds.\n",
    "\n",
    "Display the table, the output should be sorted ascending by startTime.\n",
    "\n",
    "Once the stream has produced some output, call the stream shutdown function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4204d127-eb1f-4c1f-be71-dd4c638935f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Join the Kafka streaming orders dataframe to the `product.csv` dataset.  \n",
    "Note that Spark assumes that any streaming dataframes refer to a directory, not a specific file.\n",
    "\n",
    "Aggregate the data by sum(price) (`total_price`) and a 2-minute tumbling window.  \n",
    "Add a 10 minute watermark, and store the data in a delta table.\n",
    "\n",
    "Create a view on top of the delta table with the following columns:\n",
    "* product_id\n",
    "* product_name\n",
    "* n_minus_2_window_total_price\n",
    "* n_minus_1_window_total_price\n",
    "* current_window_total_price\n",
    "\n",
    "The total_price columns need to be pivoted based on only the 3 most recent windows. \n",
    "The actual \n",
    "Order the dataset by descending `current_total_price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Practice session - Structured Streaming continued",
   "notebookOrigID": 3621743426785043,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
